{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLPSJ.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPn9K9FML8cJgnnlzD8Xw7y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/woodword-0/ML-Projects/blob/main/NLPSJ.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfsK5qaARAuh"
      },
      "source": [
        "from tensorflow import  keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow_hub as hub\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet\n",
        "import string\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLBPioziRLUw"
      },
      "source": [
        "data = pd.read_csv(\"/content/mtsamples.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvVfM0wfROsz"
      },
      "source": [
        "(data.to_numpy())[3][4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5t0GG3htRUY3"
      },
      "source": [
        "df = data[['description', 'medical_specialty']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fswUAwvRqCR"
      },
      "source": [
        "specialties  = df.groupby(df['medical_specialty'])\n",
        "\n",
        "i = 1\n",
        "for specialty, number in specialties:\n",
        "    print(str(i)+' '+specialty + ' : '+ str(len(number)) )\n",
        "    i = i+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGHXBDbhTeR7"
      },
      "source": [
        "df[df['medical_specialty']==' consult - history and phy.']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_UT44yUTmwC"
      },
      "source": [
        "#convert all text into lower case\n",
        "df = df.apply(lambda x: x.astype(str).str.lower())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOu3aSiJTrNu"
      },
      "source": [
        "#dropping record with no standard specialty\n",
        "df = df.drop(df[(df.medical_specialty == ' soap / chart / progress notes') ].index)\n",
        "df = df.drop(df[(df.medical_specialty == ' office notes') ].index)\n",
        "df = df.drop(df[(df.medical_specialty == ' letters') ].index)\n",
        "df = df.drop(df[(df.medical_specialty == ' lab medicine - pathology') ].index)\n",
        "df = df.drop(df[(df.medical_specialty == ' ime-qme-work comp etc.') ].index)\n",
        "df = df.drop(df[(df.medical_specialty == ' emergency room reports') ].index)\n",
        "df = df.drop(df[(df.medical_specialty == ' discharge summary') ].index)\n",
        "df = df.drop(df[(df.medical_specialty == ' consult - history and phy.') ].index)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IneHr4W2T12l"
      },
      "source": [
        "#dropping specialties with records<50\n",
        "df = df.drop(df[(df.medical_specialty == ' podiatry') ].index)\n",
        "df = df.drop(df[(df.medical_specialty == ' dermatology') ].index)\n",
        "df = df.drop(df[(df.medical_specialty == ' cosmetic/plastic surgery') ].index)\n",
        "df = df.drop(df[(df.medical_specialty == ' dentistry') ].index)\n",
        "df = df.drop(df[(df.medical_specialty == ' physical medicine - rehab') ].index)\n",
        "df = df.drop(df[(df.medical_specialty == ' sleep medicine') ].index)\n",
        "df = df.drop(df[(df.medical_specialty == ' endocrinology') ].index)\n",
        "df = df.drop(df[(df.medical_specialty == ' bariatrics') ].index)\n",
        "df = df.drop(df[(df.medical_specialty == ' chiropractic') ].index)\n",
        "df = df.drop(df[(df.medical_specialty == ' diets and nutritions') ].index)\n",
        "df = df.drop(df[(df.medical_specialty == ' rheumatology') ].index)\n",
        "df = df.drop(df[(df.medical_specialty == ' speech - language') ].index)\n",
        "df = df.drop(df[(df.medical_specialty == ' autopsy') ].index)\n",
        "df = df.drop(df[(df.medical_specialty == ' allergy / immunology') ].index)\n",
        "df = df.drop(df[(df.medical_specialty == ' hospice - palliative care') ].index)\n",
        "df = df.drop(df[(df.medical_specialty == ' surgery') ].index)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAFp37j5T5uk"
      },
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "sns.countplot(y='medical_specialty', data = df )\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vd07bYC9ykEG"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6shCbWvT5y1"
      },
      "source": [
        "def clean_text(description_): \n",
        "    description_ = description_.translate(str.maketrans('', '', string.punctuation))\n",
        "    description_1 = ''.join([i for i in description_ if not i.isdigit()])\n",
        "    \n",
        "\n",
        "    return description_1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RYFdmMcT525"
      },
      "source": [
        "#remove all punctuations and digits from the description column\n",
        "df['description'] = df['description'].apply(clean_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgUq4gYkT56Z"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bn9ZNuH9T59S"
      },
      "source": [
        "#Lemmatizing with appropriate POS tag\n",
        "\n",
        "def get_wordnet_pos(word):\n",
        "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "\n",
        "def lemmatize_text(description_):\n",
        "    individual_word_list=[]\n",
        "    lemmatizer = WordNetLemmatizer() \n",
        "    words = word_tokenize(description_)\n",
        "    stop_words = stopwords.words('english')\n",
        "\n",
        "    words_without_sw = [word for word in words if not word in stop_words]\n",
        "\n",
        "    for word in words_without_sw:\n",
        "      individual_word_list.append(lemmatizer.lemmatize(word, get_wordnet_pos(word)))\n",
        "           \n",
        "    return ' '.join(individual_word_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIZQZ1d8T6AK"
      },
      "source": [
        "#remove all punctuations and digits from the description column\n",
        "df['description'] = df['description'].apply(lemmatize_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRFEMHmtT6CZ"
      },
      "source": [
        " from collections import Counter\n",
        " from tqdm import tqdm\n",
        " import math\n",
        " import operator\n",
        " import numpy as np \n",
        " from scipy.sparse import csr_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysMna01kT6HO"
      },
      "source": [
        "corpus_arr = df['description'].to_numpy()\n",
        "corpus = corpus_arr.flatten().tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTm5tHV3UNSe"
      },
      "source": [
        "def IDF(corpus, unique_words):\n",
        "  idf_dict={}\n",
        "  N=len(corpus)\n",
        "  for i in unique_words:\n",
        "    count=0\n",
        "    for sen in corpus:\n",
        "      if i in sen.split():\n",
        "        count=count+1\n",
        "      idf_dict[i]=(math.log((1+N)/(count+1)))+1\n",
        "  return idf_dict "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5MUOBWbUNU5"
      },
      "source": [
        "def fit(whole_data):\n",
        "  unique_words = set()\n",
        "  if isinstance(whole_data, (list,)):\n",
        "    for x in whole_data:\n",
        "      for y in x.split():\n",
        "        if len(y)<2:\n",
        "          continue\n",
        "        unique_words.add(y)\n",
        "    unique_words = sorted(list(unique_words))\n",
        "    vocab = {j:i for i,j in enumerate(unique_words)}\n",
        "    Idf_values_of_all_unique_words=IDF(whole_data,unique_words)\n",
        "    \n",
        "    return vocab, Idf_values_of_all_unique_words\n",
        "    \n",
        "Vocabulary, idf_of_vocabulary=fit(corpus) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcv-WiTLUNY8"
      },
      "source": [
        " def transform(dataset,vocabulary,idf_values):\n",
        "     sparse_matrix= csr_matrix( (len(dataset), len(vocabulary)), dtype=np.float64)\n",
        "     for row  in range(0,len(dataset)):\n",
        "       number_of_words_in_sentence=Counter(dataset[row].split())\n",
        "       for word in dataset[row].split():\n",
        "           if word in  list(vocabulary.keys()):\n",
        "               tf_idf_value=(number_of_words_in_sentence[word]/len(dataset[row].split()))*(idf_values[word])\n",
        "               sparse_matrix[row,vocabulary[word]]=tf_idf_value\n",
        "     return sparse_matrix\n",
        " final_output=transform(corpus,Vocabulary,idf_of_vocabulary)\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cT_LKPOVUNby"
      },
      "source": [
        "set(df['medical_specialty'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQ5h0P5wUNer"
      },
      "source": [
        "df['medical_specialty_code'] = df['medical_specialty']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vo7hvTrU1rx"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPTXZH9BZ1qH"
      },
      "source": [
        "df['medical_specialty'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vy_0IIB9UNhv"
      },
      "source": [
        "#manual encoding of y variable classes\n",
        "df['medical_specialty_code'] = df['medical_specialty_code'].replace(' cardiovascular / pulmonary', 1)\n",
        "df['medical_specialty_code'] = df['medical_specialty_code'].replace(' cosmetic / plastic surgery', 2)\n",
        "df['medical_specialty_code'] = df['medical_specialty_code'].replace(' ent - otolaryngology', 3)\n",
        "df['medical_specialty_code'] = df['medical_specialty_code'].replace(' gastroenterology', 4)\n",
        "df['medical_specialty_code'] = df['medical_specialty_code'].replace(' general medicine', 5)\n",
        "df['medical_specialty_code'] = df['medical_specialty_code'].replace(' hematology - oncology', 6)\n",
        "df['medical_specialty_code'] = df['medical_specialty_code'].replace(' nephrology', 7)\n",
        "df['medical_specialty_code'] = df['medical_specialty_code'].replace(' neurology', 8)\n",
        "df['medical_specialty_code'] = df['medical_specialty_code'].replace(' neurosurgery', 9)\n",
        "df['medical_specialty_code'] = df['medical_specialty_code'].replace(' obstetrics / gynecology', 10)\n",
        "df['medical_specialty_code'] = df['medical_specialty_code'].replace(' ophthalmology', 11)\n",
        "df['medical_specialty_code'] = df['medical_specialty_code'].replace(' orthopedic', 12)\n",
        "df['medical_specialty_code'] = df['medical_specialty_code'].replace(' pain management', 13)\n",
        "df['medical_specialty_code'] = df['medical_specialty_code'].replace(' pediatrics - neonatal', 14)\n",
        "df['medical_specialty_code'] = df['medical_specialty_code'].replace(' psychiatry / psychology', 15)\n",
        "df['medical_specialty_code'] = df['medical_specialty_code'].replace(' radiology', 16)\n",
        "df['medical_specialty_code'] = df['medical_specialty_code'].replace(' urology', 17)\n",
        "# df['medical_specialty_code'] = df['medical_specialty_code'].replace(' surgery',0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVlFF2ebUmrS"
      },
      "source": [
        "set(df.medical_specialty_code)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUaTCe6tqkKb"
      },
      "source": [
        "df.sample(frac=1)\n",
        "df['medical_specialty_code'].sample(frac=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZatANLG-PwM"
      },
      "source": [
        "enc_nom_1 = (df.groupby('description').size()) \n",
        "enc_nom_1\n",
        "df['description'] = df['description'].apply(lambda x : enc_nom_1[x])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXw1iU645vr3"
      },
      "source": [
        "X = df.drop(['medical_specialty'], axis=1)\n",
        "X1_train = X.iloc[:3033].to_numpy().reshape((1,-1))\n",
        "X1_validate = X[3033:3502].to_numpy()#.reshape((1,-1))\n",
        "X1_test = X[3502:3791].to_numpy()#.reshape((1,-1))\n",
        "y1_train = df['medical_specialty_code'].iloc[:3303].to_numpy().reshape((1,-1))\n",
        "y1_validate = df['medical_specialty_code'].iloc[3033:3502].to_numpy()\n",
        "y1_test = df['medical_specialty_code'].iloc[3502:3791].to_numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZoGrf-U97p8",
        "outputId": "44709fce-98d6-4aa1-a56d-477d7a532550"
      },
      "source": [
        "X1_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 5376)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 554
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwi5q4hIlymx",
        "outputId": "5e10f2f3-da44-4d8f-f277-79b7cade77b5"
      },
      "source": [
        "y1_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([], dtype=int64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 555
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRRb9SzJlRua"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2uTbP4gWzvc"
      },
      "source": [
        "# final_output_arr = final_output.toarray()\n",
        "\n",
        "# X_train = final_output_arr[:3033]\n",
        "# X_val = final_output_arr[3033:3502]\n",
        "# X_test = final_output_arr[3502:3791]\n",
        "# y_train = df['medical_specialty_code'].iloc[:3033].to_numpy()#.reshape((-1,1))\n",
        "# y_val = df['medical_specialty_code'].iloc[3033:3502].to_numpy()#.reshape((-1,1))\n",
        "# y_test = df['medical_specialty_code'].iloc[3502:3791].to_numpy()#.reshape((-1,1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KB2UKDLAVVZv"
      },
      "source": [
        "final_output_arr = final_output.toarray()\n",
        "\n",
        "# Define a size for your train set \n",
        "train_size = int(0.8* len(df))\n",
        "val_size = int(0.1* len(df))\n",
        "test_size = int(0.1* len(df))\n",
        "\n",
        "\n",
        "X_train = final_output_arr[:train_size]\n",
        "X_val = final_output_arr[train_size:(train_size+val_size)]\n",
        "X_test = final_output_arr[(train_size+val_size):]\n",
        "y_train = df['medical_specialty_code'].iloc[:train_size].to_numpy()#.reshape((-1,1))\n",
        "y_val = df['medical_specialty_code'].iloc[train_size:(train_size+val_size)].to_numpy()#.reshape((-1,1))\n",
        "y_test = df['medical_specialty_code'].iloc[(train_size+val_size):].to_numpy()#.reshape((-1,1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHtMlfe996Uv",
        "outputId": "e3dfc56f-6f7c-4b04-d9e6-806bb5e66933"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2150, 4667)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 558
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CsqcXi7ZMvp"
      },
      "source": [
        "y_val\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q30rzwf1Uxzy"
      },
      "source": [
        "def softmax(h):\n",
        "  return (np.exp(h.T)/np.sum(np.exp(h), axis=1)).T\n",
        "\n",
        "\n",
        "def cross_entropy(Y,P_hat):\n",
        "  return -(1/len(Y))* np.sum(np.sum(Y*np.log(P_hat), axis=1),axis=0)\n",
        "\n",
        "\n",
        "def accuracy(y, y_hat):\n",
        "  return np.mean(y == y_hat)\n",
        "\n",
        "def indices_to_one_hot(data, nb_classes):\n",
        "  targets = np.array(data).reshape(-1)\n",
        "  return np.eye(nb_classes)[targets]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgOms3WuUyvv"
      },
      "source": [
        "class MVLogisticRegression():\n",
        "  def __init__(self,thresh=0.5):\n",
        "    self.thresh = thresh\n",
        "\n",
        "  \n",
        "  def fit(self, X, y, eta=2e-1, epochs=1e3, show_curve = False):\n",
        "    epochs = int(epochs)\n",
        "    N,D = X.shape\n",
        "\n",
        "    K = len(np.unique(y)) + 3\n",
        "    y_values = np.unique(y, return_index = False)\n",
        "    Y = indices_to_one_hot(y, K).astype(int)\n",
        "    self.W = np.random.randn(D, K)\n",
        "    self.B = np.random.rand(1, K)\n",
        "    J = np.zeros(int(epochs))\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "      P_hat = self.__forward__(X)\n",
        "      J[epoch]=cross_entropy(Y, P_hat)\n",
        "      self.W -= eta*(1/N)*X.T@(P_hat - Y)\n",
        "      self.B -= eta*(1/N)*np.sum(P_hat - Y)\n",
        "\n",
        "\n",
        "    if show_curve:\n",
        "      plt.figure()\n",
        "      plt.plot(J)\n",
        "      plt.xlabel('epochs')\n",
        "      plt.ylabel(\"$\\matchcal{J}\")\n",
        "      plt.title(\"Training Curve\")\n",
        "      plt.show()\n",
        "    \n",
        "  def __forward__(self, X):\n",
        "    return softmax(X@self.W + self.B)\n",
        "\n",
        "  def predict(self, X):\n",
        "    return np.argmax(self.__forward__(X), axis = 1)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCS225K47fPl"
      },
      "source": [
        "Model 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6s0ty60cU2ds"
      },
      "source": [
        "logreg = MVLogisticRegression()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYsizqucU5QE"
      },
      "source": [
        "logreg.fit(X_train,y_train,eta=5e-1, epochs=1e4, show_curve=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeR2kPfT7NyX"
      },
      "source": [
        "y_hat = logreg.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWVvb2om7Ypz"
      },
      "source": [
        "accuracy(y_test,y_hat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9M_KHapOXGbB"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARmfLMsp7khj"
      },
      "source": [
        "Model 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-97KAZExZEF5"
      },
      "source": [
        "model = tf.keras.Sequential()\n",
        "# model.add(hub_layer)\n",
        "model.add(tf.keras.layers.Dense(64,activation='relu',input_dim = 4667))\n",
        "model.add(tf.keras.layers.Dense(32,activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(16,activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(32,activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(18,activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVJBDI71eVh5"
      },
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
        "\n",
        "# model.compile(optimizer='Adam',loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6LGwUlTecF0"
      },
      "source": [
        "history=model.fit(X_train,y_train,epochs=20, validation_data=(X_val, y_val),verbose=1,validation_split=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjCxIiYXaJ7b"
      },
      "source": [
        "results=model.evaluate(X_test,y_test, verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMDnvK_8Nkqs"
      },
      "source": [
        "y_hat = model.predict(X_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xI0he-MUZ_Rg",
        "outputId": "ad444eba-c9b6-4fed-c0e6-475101eaeb05"
      },
      "source": [
        "y_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(270,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 572
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYpuN2mkZ5tg",
        "outputId": "079d57da-8624-424f-d17f-ce25703e30ce"
      },
      "source": [
        "y_hat.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(270, 18)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 573
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5icRx0WD5Br"
      },
      "source": [
        "def accuracy(y, y_hat):\n",
        "  return np.mean(y == y_hat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjN1VQ88HGBc"
      },
      "source": [
        "accuracy(y_test, y_hat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5WphEaYHTg_"
      },
      "source": [
        "prediction = model.predict(transform(X_train,'stomach pain',idf_values))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynuDAFXYc1O2"
      },
      "source": [
        "prediction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Yykz86E7pj6"
      },
      "source": [
        "Model 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98cUldyAoizY"
      },
      "source": [
        "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
        "hub_layer = hub.KerasLayer(embed, input_shape = [], dtype = tf.string, trainable= True) #trainable freezes weights turn on or off #Tokenizing Layer\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikG5HPHoSY3d"
      },
      "source": [
        "model = tf.keras.Sequential()\n",
        "model.add(hub_layer)\n",
        "model.add(tf.keras.layers.Dense(4667,activation = 'relu')) \n",
        "model.add(tf.keras.layers.Dense(19, activation = \"softmax\")) #here we build a simple classifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FCdYk4G6r7Z"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7k6YU9gy6xuu"
      },
      "source": [
        "model.compile(optimizer='Adam', loss='sparse_categorical_crossentropy', metrics= 'accuracy'\n",
        "              ) #from logits takes into account probability"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5nQQ1CI63kh"
      },
      "source": [
        "history = model.fit(X1_train,y1_train.T, epochs = 20,verbose = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vz9MKIUdflpn"
      },
      "source": [
        "# X = np.random.randint(0,10, (1000,100))\n",
        "# y = np.random.randint(0,3, 1000)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    Dense(128, input_dim = 4781),\n",
        "    Dense(18, activation='softmax'),\n",
        "])\n",
        "model.summary()\n",
        "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "history = model.fit(X_train, y_train, epochs=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxprwyQ_iMTH"
      },
      "source": [
        "X = np.random.randint(0,10, (1000,100))\n",
        "y = np.random.randint(0,3, 1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1STocKCinKf"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjcJXKfOiNhS"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oL_5JBPKiOdu"
      },
      "source": [
        "y.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McianwAciQ1Z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}