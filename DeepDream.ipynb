{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepDream.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMaSx6HqVSHxZaWrQ2TF4Ch",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/woodword-0/ML-Projects/blob/main/DeepDream.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVKOyL-hsXTT"
      },
      "source": [
        "!pip install image\n",
        "!pip install pillow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01YLAvu7tNj5"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import IPython.display as display\n",
        "# import PIL.image\n",
        "from tensorflow.keras.preprocessing import image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBJD5AyJ0Rw2"
      },
      "source": [
        "import PIL\n",
        "from PIL import Image\n",
        "print('Pillow Version:', PIL.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xeQfwIQtIKi",
        "outputId": "ec2bfc99-56a2-42ee-cebc-4073e486dbb6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5WaRLm7NV6L"
      },
      "source": [
        "im = Image.open('/Ellie.png')\n",
        "rgb_im = im.convert('RGB')\n",
        "rgb_im.save('Ellie.jpg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeazAJr28rEp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8GVyiTeyRro"
      },
      "source": [
        "url = '/Ellie.png'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pl2HGJIFzONJ"
      },
      "source": [
        "PIL.Image.open(url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvTKeh_vyw3h"
      },
      "source": [
        "#Downloading an image and read it into a numpy array\n",
        "def download(url,max_dim = None):\n",
        "  name =  url.split('/')[-1]\n",
        "  image_path = '/content/Ellie.jpg'\n",
        "  img = PIL.Image.open(image_path)\n",
        "  if max_dim:\n",
        "    img.thumbnail((max_dim, max_dim))\n",
        "  return np.array(img)\n",
        "#Normalize an image\n",
        "def deprocess(img):\n",
        "  img = 255*(img + 1.0)/2.0\n",
        "  return tf.cast(img, tf.uint8)\n",
        "#Display an image\n",
        "def show(img):\n",
        "  display.display(PIL.Image.fromarray(np.array(img)))\n",
        "#Downsizing the image making it easier to work with\n",
        "original_img = download(url, max_dim = 500)\n",
        "show(original_img)\n",
        "display.display(url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0XLBWbB3uZ8"
      },
      "source": [
        "Prepare the feature extraction model\n",
        "We will download and prepare a pretrained classification model InceptionV3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwG9My4418o-"
      },
      "source": [
        "base_model  = tf.keras.applications.InceptionV3(include_top= False, weights = 'imagenet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5BEsi6L4hLH"
      },
      "source": [
        "#Maximize the activation of these layers\n",
        "names = ['mixed4', 'mixed6'] #mixed3 mixed 5\n",
        "layers = [base_model.get_layer(name).output for name in names]\n",
        "#Create the feature extraction model\n",
        "dream_model = tf.keras.Model(inputs = base_model.input, outputs = layers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MVGQl7Q6uEw"
      },
      "source": [
        "Calculate Loss: Loss is the sum of the activations in the chosen layers. We want to maximize loss here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bxFA3Cu6QU2"
      },
      "source": [
        "def calc_loss(img, model):\n",
        "  #Pass forward the image through the model to retrieve the activations\n",
        "  #Converts the image into a batch of size 1\n",
        "  img_batch = tf.expand_dims(img, axis = 0)\n",
        "  layer_activations = model(img_batch)\n",
        "  if len(layer_activations) == 1:\n",
        "    layer_activations = [layer_activations]\n",
        "  losses = []\n",
        "  for act in layer_activations:\n",
        "    loss = tf.math.reduce_mean(act)\n",
        "    losses.append(loss)\n",
        "  return tf.reduce_sum(losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbaTuV9O8-SW"
      },
      "source": [
        "Gradient Ascent: After calculating loss, we now just need to calculate gradients with respect to the image and add to the original image. Adding gradients enhances the patterns seen by the network "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_qFng6V9eS8"
      },
      "source": [
        "class DeepDream(tf.Module):\n",
        "  def __init__(self, model):\n",
        "    self.model = model\n",
        "  @tf.function(\n",
        "      input_signature = (\n",
        "          tf.TensorSpec(shape = [None, None, 3], dtype = tf.float32), \n",
        "          tf.TensorSpec(shape = [], dtype = tf.int32), \n",
        "          tf.TensorSpec(shape = [], dtype = tf.float32))\n",
        "  )\n",
        "  def __call__(self, img, steps, step_size):\n",
        "    print('Tracing')\n",
        "    loss = tf.constant(0.0)\n",
        "    for n in tf.range(steps):\n",
        "      with tf.GradientTape() as tape:\n",
        "        #This needs gradients relative to the image\n",
        "        #GradientTape only watches tf.Variables \n",
        "        tape.watch(img)\n",
        "        loss = calc_loss(img, self.model)\n",
        "      #Calculate the gradient of the loss with respect to the pixels of the input image\n",
        "      gradients = tape.gradient(loss, img)\n",
        "      #Normalize the gradients\n",
        "      gradients /= tf.math.reduce_std(gradients) + 1e-8 #computes std of tensor elements\n",
        "      #In gradient ascent, the loss is maximized so that the input image\n",
        "      #increasingly excites the layers\n",
        "      #We can update the image by directly adding the gradients\n",
        "      #(since they are the same shape)\n",
        "      img = img + gradients*step_size\n",
        "      img = tf.clip_by_value(img, -1, 1) #Clips tensor values to a specified min and max.\n",
        "    return loss, img\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0HaOpIX88Ob"
      },
      "source": [
        "deepdream = DeepDream(dream_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QI5ZHWorCD2d"
      },
      "source": [
        "Main Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLdiB8lkCFQo"
      },
      "source": [
        "def run_deep_dream_simple(img, steps = 100, step_size = 0.01):\n",
        "  #convert from uint8 to the range expected by the model.\n",
        "  img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
        "  img = tf.convert_to_tensor(img)\n",
        "  step_size = tf.convert_to_tensor(step_size)\n",
        "  steps_remaining  = steps\n",
        "  step = 0\n",
        "  while steps_remaining:\n",
        "    if steps_remaining > 100:\n",
        "      run_steps = tf.constant(100)\n",
        "    else:\n",
        "      run_steps = tf.constant(steps_remaining)\n",
        "    steps_remaining -= run_steps\n",
        "    step += run_steps\n",
        "\n",
        "    loss, img = deepdream(img, run_steps, tf.constant(step_size))\n",
        "    display.clear_output(wait = True)\n",
        "    show(deprocess(img))\n",
        "    print(\"Step {}, loss {}\".format(step, loss))\n",
        "  result = deprocess(img)\n",
        "  display.clear_output(wait = True)\n",
        "  show(result)\n",
        "\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXeRdfeNCCEZ"
      },
      "source": [
        "dream_img = run_deep_dream_simple(img = original_img, steps = 100, step_size = 0.01)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GaVAgndFRts"
      },
      "source": [
        "Output is noisy\n",
        "Image is Low resolution\n",
        "Patterns appear all a the same grainularity\n",
        "To solve these problems, apply gradient ascent at different scales"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ebtJczHFjUK"
      },
      "source": [
        "import time\n",
        "start = time.time()\n",
        "OCTAVE_SCALE = 1.30\n",
        "img = tf.constant(np.array(original_img))\n",
        "base_shape = tf.shape(img)[:-1]\n",
        "float_base_shape = tf.cast(base_shape, tf.float32)\n",
        "for n in range(-2,3):\n",
        "  new_shape = tf.cast(float_base_shape*(OCTAVE_SCALE**n), tf.int32)\n",
        "  img = tf.image.resize(img, new_shape).numpy()\n",
        "  img = run_deep_dream_simple(img = img, steps = 50, step_size=0.01)\n",
        "\n",
        "display.clear_output(wait = True)\n",
        "img = tf.image.resize(img, base_shape)\n",
        "img = tf.image.convert_image_dtype(img/255.0, dtype=tf.uint8)\n",
        "show(img)\n",
        "\n",
        "end = time.time()\n",
        "end-start"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MptdUe_Mdz_9"
      },
      "source": [
        "To deal with large images, we split the image into windows and perform a gradient on each tile"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFBe60g5EXsm"
      },
      "source": [
        "def random_roll(img, maxroll):\n",
        "  #randomly shift the image to avoid tiled boundaries\n",
        "  shift = tf.random.uniform(shape = [2], minval =- maxroll, maxval = maxroll, dtype = tf.int32)\n",
        "  img_rolled = tf.roll(img, shift = shift, axis = [0,1])\n",
        "  return shift, img_rolled"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lO9WNkTygBRG"
      },
      "source": [
        "shift, img_rolled = random_roll(np.array(original_img), 512)\n",
        "show(img_rolled)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XcHksl9js2o"
      },
      "source": [
        "A tiled equivalent of the deepdream function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7XBntYDgPtT"
      },
      "source": [
        "class TiledGradients(tf.Module):\n",
        "  def __init__(self, model):\n",
        "    self.model = model\n",
        "\n",
        "  @tf.function(\n",
        "      input_signature=(\n",
        "        tf.TensorSpec(shape=[None,None,3], dtype=tf.float32),\n",
        "        tf.TensorSpec(shape=[], dtype=tf.int32),)\n",
        "  )\n",
        "  def __call__(self, img, tile_size=512):\n",
        "    shift, img_rolled = random_roll(img, tile_size)\n",
        "\n",
        "    # Initialize the image gradients to zero.\n",
        "    gradients = tf.zeros_like(img_rolled)\n",
        "\n",
        "    # Skip the last tile, unless there's only one tile.\n",
        "    xs = tf.range(0, img_rolled.shape[0], tile_size)[:-1]\n",
        "    if not tf.cast(len(xs), bool):\n",
        "      xs = tf.constant([0])\n",
        "    ys = tf.range(0, img_rolled.shape[1], tile_size)[:-1]\n",
        "    if not tf.cast(len(ys), bool):\n",
        "      ys = tf.constant([0])\n",
        "\n",
        "    for x in xs:\n",
        "      for y in ys:\n",
        "        # Calculate the gradients for this tile.\n",
        "        with tf.GradientTape() as tape:\n",
        "          # This needs gradients relative to `img_rolled`.\n",
        "          # `GradientTape` only watches `tf.Variable`s by default.\n",
        "          tape.watch(img_rolled)\n",
        "\n",
        "          # Extract a tile out of the image.\n",
        "          img_tile = img_rolled[x:x+tile_size, y:y+tile_size]\n",
        "          loss = calc_loss(img_tile, self.model)\n",
        "\n",
        "        # Update the image gradients for this tile.\n",
        "        gradients = gradients + tape.gradient(loss, img_rolled)\n",
        "\n",
        "    # Undo the random shift applied to the image and its gradients.\n",
        "    gradients = tf.roll(gradients, shift=-shift, axis=[0,1])\n",
        "\n",
        "    # Normalize the gradients.\n",
        "    gradients /= tf.math.reduce_std(gradients) + 1e-8 \n",
        "\n",
        "    return gradients"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-feWuasojzA"
      },
      "source": [
        "get_tiled_gradients = TiledGradients(dream_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5pCYx0Sotw3"
      },
      "source": [
        "Putting this all together gives a scalable, octave-aware deepdream implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcsfVD-GooB2"
      },
      "source": [
        "def run_deep_dream_with_octaves(img, steps_per_octave=100, step_size=0.01, \n",
        "                                octaves=range(-2,3), octave_scale=1.3):\n",
        "  base_shape = tf.shape(img)\n",
        "  img = tf.keras.preprocessing.image.img_to_array(img)\n",
        "  img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
        "\n",
        "  initial_shape = img.shape[:-1]\n",
        "  img = tf.image.resize(img, initial_shape)\n",
        "  for octave in octaves:\n",
        "    # Scale the image based on the octave\n",
        "    new_size = tf.cast(tf.convert_to_tensor(base_shape[:-1]), tf.float32)*(octave_scale**octave)\n",
        "    img = tf.image.resize(img, tf.cast(new_size, tf.int32))\n",
        "\n",
        "    for step in range(steps_per_octave):\n",
        "      gradients = get_tiled_gradients(img)\n",
        "      img = img + gradients*step_size\n",
        "      img = tf.clip_by_value(img, -1, 1)\n",
        "\n",
        "      if step % 10 == 0:\n",
        "        display.clear_output(wait=True)\n",
        "        show(deprocess(img))\n",
        "        print (\"Octave {}, Step {}\".format(octave, step))\n",
        "\n",
        "  result = deprocess(img)\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQnT4gEisEwg"
      },
      "source": [
        "img = run_deep_dream_with_octaves(img=original_img, step_size=0.01)\n",
        "\n",
        "display.clear_output(wait=True)\n",
        "img = tf.image.resize(img, base_shape)\n",
        "img = tf.image.convert_image_dtype(img/255.0, dtype=tf.uint8)\n",
        "show(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1QcbB9Tsk6o"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}